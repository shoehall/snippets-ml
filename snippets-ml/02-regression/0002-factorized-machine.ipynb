{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FM(Factorized Machine)因子分解机\n",
    "\n",
    "该方法常用的场景是推荐，在snippets-rec中会有FM及其衍生算法的介绍。从ml的角度看，该算法不仅仅是一个回归算法, 也可以用于分类、排序等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 简介\n",
    "FM可以视为线性回归引入特征交叉的一种演进:\n",
    "\n",
    "1)线性回归引入特征交叉\n",
    "$$\n",
    "\\hat{y}(x)=w_0 + \\underbrace{\\sum_{i=1}^p w_i x_i}_{\\text {线性回归 }}+\\underbrace{\\sum_{i=1}^p \\sum_{j=i+1}^p w_{i j} x_i x_j}_{\\text {交叉项 (组合待征) }}\n",
    "$$\n",
    "\n",
    "其中样本$x_i$为样本x的第$i$维特征，$p$为维数.\n",
    "但在当x为稀疏数据时（如商品类别或标签）维度可能有几万甚至上百万。交叉项会有维度爆炸，同时绝大多数维度是不相关的，导致交叉项非常稀疏，很难学习。因此FM通过矩阵分解对矩阵$V$进行了近似.\n",
    "\n",
    "2)FM: 通过矩阵分解对交叉项进行近似\n",
    "$$\n",
    "\\begin{aligned}\n",
    "交叉相部分= \\\\\n",
    "& \\sum_{i=1}^{p} \\sum_{j=i+1}^p\\left\\langle\\mathbf{v}_i, \\mathbf{v}_j\\right\\rangle x_i x_j \\\\\n",
    "= & \\frac{1}{2} \\sum_{i=1}^p \\sum_{j=1}^p\\left\\langle\\mathbf{v}_i, \\mathbf{v}_j\\right\\rangle x_i x_j-\\frac{1}{2} \\sum_{i=1}^p\\left\\langle\\mathbf{v}_i, \\mathbf{v}_i\\right\\rangle x_i x_i \\\\\n",
    "= & \\frac{1}{2}\\left(\\sum_{i=1}^p \\sum_{j=1}^p \\sum_{f=1}^k v_{i, f} v_{j, f} x_i x_j-\\sum_{i=1}^p \\sum_{f=1}^k v_{i, f} v_{i, f} x_i x_i\\right) \\\\\n",
    "= & \\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{i=1}^{p} v_{i, f} x_i\\right)\\left(\\sum_{j=1}^{p} v_{j, f} x_j\\right)-\\sum_{i=1}^{p} v_{i, f}^2 x_i^{2}\\right) \\\\\n",
    "= & \\frac{1}{2} \\sum_{f=1}^k\\left(\\left ( \\sum_{i=1}^{p} v_{i, f} x_i \\right)^{2}-\\sum_{i=1}^{p} v_{i, f}^2 x_i^{2}\\right)\n",
    "\\end{aligned}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "上述为y关于x的分布的表达式（还缺一个残差项，如正态分布的残差）。\n",
    "下面我们将上述表达式以样本形式矩阵化，方便机器学习和深度学习引擎求解，通过下面几步阐述：\n",
    "step 1)假定共有$n$个样本，即$X \\in \\mathbb{R}^{n \\times p}$，$Y \\in \\mathbb{R}^{n}$\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_1^{(1)} & \\dots & x_p^{(1)}\\\\\n",
    " \\vdots \\ & \\ddots \\ & \\vdots \\\\\n",
    "x_1^{(n)} & \\dots & x_p^{(n)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "step 2)$V \\in \\mathbb{R}^{p \\times k}$\n",
    "$$\n",
    "V = \\begin{bmatrix}\n",
    "v_1^{(1)} & \\dots & v_k^{(1)}\\\\\n",
    " \\vdots \\ & \\ddots \\ & \\vdots \\\\\n",
    "v_1^{(p)} & \\dots & v_k^{(p)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "step 3)Y的交叉项$Y_{inter}$\n",
    "\n",
    "$$\n",
    "Y_{inter} = \\sum_{i=1}^{p} \\sum_{j=i+1}^{p} \\langle \\textbf v_i, \\textbf v_j \\rangle \\vec{x_i} \\odot \\vec{x_j}  \\\\\n",
    "= \\frac{1}{2} \\sum_{f=1}^{k} \\Big( \\big(\\sum_{i=1}^{p} v_f^{(i)} \\vec{x_i} \\big)^2 - \\sum_{i=1}^{p}v_f^{(i) 2} \\vec{x_i}^2 \\Big) \\\\\n",
    "= \\frac{1}{2} \\sum_{f=1}^{k}  \\big(\\sum_{i=1}^{p} v_f^{(i)} \\vec{x_i} \\big)^2 -\n",
    "\\frac{1}{2} \\sum_{f=1}^{k}  \\big( \\sum_{i=1}^{p}v_f^{(i) 2} \\vec{x_i}^2 \\big)\n",
    "$$\n",
    "\n",
    "注意：\n",
    "- 此处$\\vec{x_i} $表示向量，$\\vec{x_i} \\in \\mathbb{R}^{n \\times 1}$\n",
    "- $\\odot$表示元素乘法(element wise product)，此处平方运算$\\vec{x_i}^{2}$也表示向量的元素级平方操作.\n",
    "- 记住该公式相减的一部分和第二部分, 后续会用到.\n",
    "\n",
    "step4)Y的交叉项可以利用$XV$进一步简化:\n",
    "$X$和$V$的内积($n*k$维):\n",
    "$$\n",
    "XV = \\begin{bmatrix}\n",
    "\\sum_{i=1}^{p} v_f^{(1)} x_i^{(1)}  & \\dots &  \\sum_{i=1}^{p} v_f^{(k)} x_i^{(1)}\\\\\n",
    " \\vdots \\ & \\ddots \\ & \\vdots \\\\\n",
    "\\sum_{i=1}^{p} v_f^{(1)} x_i^{(n)} & \\dots & \\sum_{i=1}^{(n)} v_f^{(k)} x_i^{(n)} \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "S_{1,1}^{(1)}  & \\dots &  S_{1,k}^{(1)}\\\\\n",
    " \\vdots \\ & \\ddots \\ & \\vdots \\\\\n",
    "S_{1,1}^{(n)}  & \\dots & S_{1,k}^{(n)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们注意到:\n",
    "- step3中$Y$求解第一项中求和元素$\\sum_{i=1}^{p} v_f^{(i)}$和 $ \\vec{x_i} $ 的乘积恰好等于$XV$的第$i$列:\n",
    "$$\n",
    "v_f^{(i)} \\vec{x_i} = \\begin{pmatrix}\n",
    "  v_f^{(i)} x_i^{(1)}  \\\\\n",
    "  ... \\\\\n",
    "  v_f^{(i)} x_i^{(n)}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- 因此step3中第一项$\\frac{1}{2} \\sum_{f=1}^{k}  \\big(\\sum_{i=1}^{p} v_f^{(i)} \\vec{x_i} \\big)^2$等价于$XV$的元素级平方$(XV)^2 = (XV) \\odot (XV)$.\n",
    "我们定义一个运算表示对矩阵的列求和: $\\sigma_{(1)}(B)$表示对矩阵$B$的列向量求和. 因此有:\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{f=1}^{k}  \\big(\\sum_{i=1}^{p} v_f^{(i)} \\vec{x_i} \\big)^2\n",
    "=\\frac{1}{2} \\sigma_{(1)}((XV) \\odot (XV))\n",
    "$$\n",
    "\n",
    "- 同理step3中的第二项中求和元素$ \\sum_{i=1}^{p}v_f^{(i) 2} \\vec{x_i}^2$可以视为$(X \\odot X) \\dot (V \\odot V)$的第$i$列, 第二项可以视为$(X \\odot X) \\dot (V \\odot V)$的列向量求和:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{f=1}^{k}  \\big( \\sum_{i=1}^{p}v_f^{(i) 2} \\vec{x_i}^2 \\big)\n",
    "=\\frac{1}{2} \\sigma_{(1)}((X \\odot X) \\dot (V \\odot V))\n",
    "$$\n",
    "\n",
    "因此最终的矩阵表达式为:\n",
    "\n",
    "$$\n",
    "Y = w_0 + X w + \\frac{1}{2} \\sigma_{(1)}((XV) \\odot (XV)) - \\frac{1}{2} \\sigma_{(1)}((X \\odot X) \\dot (V \\odot V))\n",
    "$$\n",
    "其中:\n",
    "- $w_0$为截距项;\n",
    "- $w$为线性项系数;\n",
    "- $V$为交叉项的隐式特征;\n",
    "- $\\odot$为元素级乘法;\n",
    "- $\\sigma_{(1)}(B)$表示对矩阵$B$的列向量求和\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TorchFM(torch.nn.Module):\n",
    "    def __init__(self, n=None, k=None):\n",
    "        super().__init__()\n",
    "        self.V = torch.nn.Parameter(torch.randn(n, k), requires_grad=True)\n",
    "        self.linear = torch.nn.Linear(n, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True)  #S_1^2\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True)  # S_2\n",
    "\n",
    "        out_inter = 0.5 * (out_1 - out_2)\n",
    "        out_linear = self.linear(x)\n",
    "        out = out_inter + out_linear\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "vv = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  4,  9],\n        [16, 25, 36],\n        [49, 64, 81]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv.pow(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}